{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f147a04-157d-4b0b-ab7a-65faac89d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- configuration_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- modeling_minicpmv.py\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:51<00:00,  7.35s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.51it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniCPMV(\n",
       "  (llm): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (vpm): Idefics2VisionTransformer(\n",
       "    (embeddings): Idefics2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      (position_embedding): Embedding(4900, 1152)\n",
       "    )\n",
       "    (encoder): Idefics2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x Idefics2EncoderLayer(\n",
       "          (self_attn): Idefics2VisionAttention(\n",
       "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Idefics2VisionMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (resampler): Resampler(\n",
       "    (kv_proj): Linear(in_features=1152, out_features=4096, bias=False)\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (ln_q): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_kv): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_post): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f4fa0b-68a5-42e1-bf80-fef41041ba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像中的文字内容为'时光如梭，期末已至,愿你的努力都能化作考试中智慧展示出无比辉煌的实力!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device='cpu', dtype=torch.bfloat16)\n",
    "\n",
    "image = Image.open('Picture7.png').convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b32d458-9f57-4187-b0f8-3098290f475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right text box contains information about concerts in Fortnite. It mentions that concerts are huge events that gather the Fortnite's artist community, are scheduled and announced in advance, and allow a maximum number of participants. It also states that these events are available by launching a game of Battle Royale, players are united to watch a virtual concert by a real artist. The game then pauses, and it is impossible to kill another player; the event brings new skins, emotes, and items for immersion and customization."
     ]
    }
   ],
   "source": [
    "model = model.to(device='cpu', dtype=torch.bfloat16)\n",
    "image = Image.open('Capture.JPG').convert('RGB')\n",
    "question = 'please describe the left part of the image'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in res:\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9be3e5-3edd-4ad3-bcb0-82783a1d4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The left side of the image features a character that appears to be a male with tattoos on his arms and chest. He is wearing a necklace with a pendant and is holding a microphone in his right hand, suggesting he is singing or performing. His expression is confident and engaging, with his mouth open as if caught mid-song. The background is dark, with purple and blue lighting, which gives the impression of a concert stage. There's also a blurred figure in the background that seems to be a crowd or an audience, indicating that this character is performing for a live audience."
     ]
    }
   ],
   "source": [
    "model = model.to(device='cpu', dtype=torch.bfloat16)\n",
    "image = Image.open('Capture.JPG').convert('RGB')\n",
    "question = 'please describe the left part of the image'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in res:\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698f1e18-451c-4fee-9fba-d47abc9ca428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a118ab2-6cfc-4e1d-be78-0c029c80c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total use 40.34442496299744 seconds\n",
      "The pie chart in the image is divided into four sections, each representing a different category of word length. The largest section, colored in blue, represents words that are more than one character long and accounts for 67% of the total. The second-largest section, colored in grey, represents non-English words and occupies 14% of the pie chart. The smallest two sections are both orange; the first represents English words with a single character, making up 2% of the chart, and the second represents English words with more than one character, which is not quantified but is presumably a smaller portion than the single-character category. The chart is set against a white background with a diagonal grid pattern."
     ]
    }
   ],
   "source": [
    "model = model.to(device='cpu', dtype=torch.bfloat16)\n",
    "image = Image.open('Picture7.png').convert('RGB')\n",
    "question = 'please describe pie chart in the image combine the legend with same color'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "begin = time()\n",
    "res = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "print(f\"Total use {time()-begin} seconds\")\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in res:\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda2921-b66f-49f0-9ef2-4ceb688c3e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
